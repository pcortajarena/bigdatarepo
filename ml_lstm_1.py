# -*- coding: utf-8 -*-
"""ML_LSTM_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OYmM1dub7fw0md8AN9H9dN_OBL2lRemx

## Imports
"""

import pandas as pd
import numpy as np

from keras.wrappers.scikit_learn import KerasClassifier

from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer

from keras.layers import Dense, LSTM, Embedding
from keras.layers import Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D, BatchNormalization
from keras.layers import Concatenate, Subtract, Multiply
from keras.layers import Input, Dropout, PReLU, SpatialDropout1D
from keras.layers import RepeatVector, TimeDistributed


from keras.models import Model, load_model, Sequential

from keras.callbacks import EarlyStopping, ModelCheckpoint

import keras.backend as K

from keras import optimizers

from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.model_selection import train_test_split

import os
import io


RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

"""## Load data"""

def load_data():
    path = os.path.join('data', 'dummy_data.csv')
    data = pd.read_csv(path)
    return data

"""## Prepare data"""

def split_train_test(dataset, test_size=0.20):
    global RANDOM_STATE
    
    train, test = train_test_split(dataset, test_size=test_size, random_state = RANDOM_STATE)
    return train, test

def split_x_y(dataset):
    train_x = dataset.drop(['y'], axis=1)
    train_y = dataset[['y']]
    return train_x, train_y
  
def reshape_3d(dataset, n_timesteps=1):
    return dataset.values.reshape((dataset.shape[0], n_timesteps, dataset.shape[1]))

"""## Model"""

def create_model(X, Y, kernel_initializer='uniform',activation='relu', optimizer='adam', weight_constraint=0, lstm_neurons=200, dense_neurons=100, n_timesteps=1):
    n_features, n_outputs = X.shape[1], Y.shape[1]
    # define model
    model = Sequential()
    model.add(LSTM(lstm_neurons, activation=activation, input_shape=(n_timesteps, n_features)))
    model.add(RepeatVector(n_outputs))
    model.add(LSTM(lstm_neurons, activation=activation, return_sequences=True))
    model.add(TimeDistributed(Dense(dense_neurons, activation=activation, kernel_initializer=kernel_initializer)))
    model.add(TimeDistributed(Dense(1)))
    model.compile(loss='mse', optimizer=optimizer)
    return model

def fit_model(model, train_x_values, train_y_values, val_x_values, val_y_values, epochs=100, batch_size=10, verbose=0, patience=5):
    early_stopping = EarlyStopping(monitor='val_mse', patience=patience)
    model_checkpoint = ModelCheckpoint('ml_lstm_1.h5', save_best_only=True, save_weights_only=False, monitor='val_loss', mode='min')
    model.fit(train_x_values, train_y_values, \
          validation_data = (val_x_values, val_y_values), \
          batch_size=batch_size, \
          epochs=epochs, \
          verbose=verbose, \
          callbacks=[early_stopping, model_checkpoint]) 
    return model

"""## Hyperparameter tuning"""

def tune_model(x, y, n_splits=10):
    global RANDOM_STATE
    # create model
    model = KerasClassifier(build_fn=create_model, X=x, Y=y)
    
    # define the grid search parameters
    kernel_initializer = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']
    activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']
    optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']
    weight_constraint = [1, 2, 3, 4, 5]
    lstm_neurons = [10, 50, 100, 150, 200]
    dense_neurons = [10, 50, 100, 150, 200]
    batch_size = [10, 20, 40, 60, 80, 100]
    epochs = [10, 50, 100]
    
    
    param_grid = dict(kernel_initializer=kernel_initializer, activation=activation, \
                      optimizer=optimizer, weight_constraint=weight_constraint,  \
                     lstm_neurons=lstm_neurons, dense_neurons=dense_neurons, \
                     batch_size=batch_size, epochs=epochs)
    
    cv = StratifiedKFold(n_splits, shuffle=True, random_state=RANDOM_STATE)
    
    grid = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        cv=cv,
        scoring='neg_mean_squared_error',
        n_jobs=-1 # in parallel
    ) 
    grid_result = grid.fit(x.values, y.values)
    # summarize results
    print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
    means = grid_result.cv_results_['mean_test_score']
    stds = grid_result.cv_results_['std_test_score']
    params = grid_result.cv_results_['params']
    for mean, stdev, param in zip(means, stds, params):
        print("%f (%f) with: %r" % (mean, stdev, param))

"""## Experiments"""

TEST_SIZE = 0.20
N_TIMESTEPS = 1
EPOCHS = 10000

dataset = load_data()
train_all, test = split_train_test(dataset, test_size=TEST_SIZE)
train_partial, validation = split_train_test(train_all, test_size=TEST_SIZE)

train_partial_x, train_partial_y = split_x_y(train_partial)
validation_x, validation_y = split_x_y(validation)

model = create_model(train_partial_x, train_partial_y, n_timesteps=N_TIMESTEPS)

train_partial_x_values = reshape_3d(train_partial_x, n_timesteps=N_TIMESTEPS)
train_partial_y_values = reshape_3d(train_partial_y, n_timesteps=N_TIMESTEPS)
validation_x_values = reshape_3d(validation_x, n_timesteps=N_TIMESTEPS)
validation_y_values = reshape_3d(validation_y, n_timesteps=N_TIMESTEPS)


model = fit_model(model, train_partial_x_values, train_partial_y_values, validation_x_values, validation_y_values, verbose=1, epochs=EPOCHS)

"""## Hyperparameter tuning experiments"""

# RuntimeError: Cannot clone object <keras.wrappers.scikit_learn.KerasClassifier object at 0x7fbeb7fc1128>, as the constructor either does not set or modifies parameter X
# train_all_x, train_all_y = split_x_y(train_all)
# 
# tune_model(train_all_x, train_all_y)

